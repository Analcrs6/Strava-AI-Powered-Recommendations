# Streamlit App Data Generation Guide

## Quick Start

The Streamlit app requires two CSV files that are NOT currently generated by the notebook:

1. **processed_activities.csv** - Activities with engineered features
2. **routes.csv** - Route metadata aggregated from activities

### Generate These Files in 30 Seconds:

```bash
cd /home/user/Strava-AI-Powered-Recommendations
python generate_csv_exports.py
```

This will create both required CSV files in the current directory.

---

## What's in Strave_recommender_ver4.ipynb

### Data Pipeline Overview

The notebook follows this data transformation pipeline:

```
synthetic_strava_data.csv (source)
    ↓ [Load with pd.read_csv()]
df (raw data: 12 columns)
    ↓ [Cell 10: Deduplicate]
df_clean (cleaned: 12 columns)
    ↓ [Cell 12: Engineer features via engineer_rich_features()]
df_enriched (processed: 32+ columns)
    ↓ [Cell 14: Group by route_id, aggregate]
route_meta (route summary: 26+ columns)
    ↓ [Cell 15: One-hot encode, standardize]
route_features (numpy array for similarity computation)
```

---

## Data Schemas

### Input: synthetic_strava_data.csv

**12 Columns:**

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| user_id | str | User identifier | "brenda53" |
| route_id | str | Route identifier | "R031" |
| distance_km_user | float | Activity distance | 10.04 |
| elevation_meters_user | float | Elevation gain | 204 |
| surface_type_user | str | Surface type | "Road", "Trail", "Mixed", "Track" |
| average_pace_min_per_km | float | Pace | 4.59 |
| rating | int | User rating | 1-5 |
| start_date | str/datetime | Activity timestamp | "2025-03-17 11:50:45" |
| distance_km_route | float | Route distance | 10.04 |
| elevation_meters_route | float | Route elevation | 204 |
| surface_type_route | str | Route surface | "Road", "Trail", "Mixed", "Track" |
| difficulty_score | float | Route difficulty | 4.04 |

**Data Statistics:**
- Multiple users (20+ synthetic profiles)
- ~100 routes (R001-R099)
- 1000+ activity records
- Ratings: mixed distribution across 1-5 scale
- Dates: spanning 2024-2025

---

### Output 1: processed_activities.csv

**32+ Columns** (all input columns + 20 engineered features)

**Original Columns (12):**
- user_id, route_id
- distance_km_user, elevation_meters_user, surface_type_user
- average_pace_min_per_km, rating, start_date
- distance_km_route, elevation_meters_route, surface_type_route, difficulty_score

**Engineered Features (20+):**

#### Terrain Features (5 columns):
- `grade_percent` - Elevation gain per km (%)
- `grade_flat` - Binary: grade < 2%
- `grade_rolling` - Binary: 2% ≤ grade < 5%
- `grade_hilly` - Binary: 5% ≤ grade < 10%
- `grade_steep` - Binary: grade ≥ 10%

#### Route Type Features (2 columns):
- `is_likely_loop` - Binary: likely loop route
- `is_likely_out_back` - Binary: likely out-and-back route

#### Environmental Features (1 column):
- `traffic_stress` - Float [0-1]: urban stress level

#### Geo Features (1 column):
- `geo_cluster` - Int [0-9]: geographic bin

#### Temporal Features (8 columns):
- `hour_of_day` - Int [0-23]: hour of activity
- `is_morning` - Binary: 5 AM - 12 PM
- `is_afternoon` - Binary: 12 PM - 5 PM
- `is_evening` - Binary: 5 PM - 10 PM
- `is_night` - Binary: 10 PM - 5 AM
- `day_of_week` - Int [0-6]: 0=Monday, 6=Sunday
- `is_weekend` - Binary: Saturday-Sunday
- `is_weekday` - Binary: Monday-Friday

**Usage:**
- Used directly by Streamlit app to display activity details
- Contains all historical data for user profiling
- One row per activity

---

### Output 2: routes.csv

**26+ Columns** (one row per unique route)

**Core Columns (1):**
- `route_id` - Route identifier

**Aggregated Numeric Features (from means across all activities):**
- `distance_km_route` - Mean distance
- `elevation_meters_route` - Mean elevation
- `difficulty_score` - Mean difficulty
- `grade_percent` - Mean grade
- `traffic_stress` - Mean stress level

**Aggregated Binary Features (from means across all activities):**
- `grade_flat` - Proportion of flat activities
- `grade_rolling` - Proportion of rolling activities
- `grade_hilly` - Proportion of hilly activities
- `grade_steep` - Proportion of steep activities
- `is_likely_loop` - Proportion classified as loops
- `is_likely_out_back` - Proportion classified as out-and-back

**Aggregated Temporal Features (from means):**
- `is_morning` - Proportion done in morning
- `is_afternoon` - Proportion done in afternoon
- `is_evening` - Proportion done in evening
- `is_night` - Proportion done at night
- `is_weekend` - Proportion done on weekends
- `is_weekday` - Proportion done on weekdays

**Categorical Feature (mode across activities):**
- `surface_type_route` - Most common surface type

**Geo Feature (mode):**
- `geo_cluster` - Most common geographic cluster

**Usage:**
- Streamlit app displays these as route metadata
- Used for recommendation filtering and similarity computation
- One row per unique route

---

## Feature Engineering Details

### When Features Are Created

**Cell 10:** Basic cleaning
```python
df_clean = (df.sort_values("start_date")
              .drop_duplicates(["user_id", "route_id"], keep="last")
              .reset_index(drop=True))
```

**Cell 12:** Feature engineering via `engineer_rich_features(df_clean)`
- Adds terrain, route type, stress, geo, and temporal features
- Operates on individual activities (rows)

**Cell 14-15:** Route aggregation
```python
route_meta = df_clean.groupby("route_id").agg({...})
```
- Aggregates features to route level
- Computes means for numeric/binary features
- Computes modes for categorical features

### Key Formulas

**Grade Percentage:**
```python
grade_percent = (elevation_meters / (distance_km * 1000 + 1e-8)) * 100
```

**Traffic Stress:**
```python
traffic_stress = surface_base_score + distance_adjustment
# surface_base: paved=0.7, gravel=0.4, dirt=0.2
# distance: + (1.0 / (distance_km + 1)) * 0.3
# clipped to [0, 1]
```

**Geo Cluster:**
```python
geo_cluster = pd.cut(distance_km * elevation_meters, bins=10, labels=False)
# Creates 10 equal-width bins across distance × elevation product
```

**Temporal Features:**
```python
hour_of_day = start_date.hour  # 0-23
is_morning = (5 <= hour < 12)  # Binary
is_morning = (day_of_week >= 5)  # Saturday=5, Sunday=6
```

---

## How to Generate the CSV Files

### Method 1: Using the Provided Script (Recommended)

```bash
python generate_csv_exports.py
```

**What it does:**
1. Loads synthetic_strava_data.csv
2. Cleans and deduplicates data
3. Engineers all 20 features
4. Aggregates to route level
5. Saves both CSV files

**Output:**
- processed_activities.csv
- routes.csv

### Method 2: From the Jupyter Notebook

Run the notebook cells in this order:

1. **Cell 1:** Setup and load libraries
2. **Cell 4:** Load data
   ```python
   df = pd.read_csv(DATA_PATH)
   ```
3. **Cell 10:** Clean data
   ```python
   df_clean = (df.sort_values("start_date")
                 .drop_duplicates(["user_id", "route_id"], keep="last")
                 .reset_index(drop=True))
   ```
4. **Cell 12:** Add features
   ```python
   df_clean = engineer_rich_features(df_clean)
   ```
5. **Cell 14-15:** Create route metadata
   ```python
   route_meta = df_clean.groupby("route_id").agg({...})
   # (see full aggregation code in notebook)
   ```

Then save:
```python
# Save processed activities
df_clean.to_csv('processed_activities.csv', index=False)

# Save route metadata
route_meta.reset_index().to_csv('routes.csv', index=False)
```

### Method 3: Custom Python Script

Copy the `engineer_rich_features()` function from Cell 12, then:

```python
import pandas as pd

def engineer_rich_features(df):
    # [Copy function from Cell 12]
    pass

# Load and process
df = pd.read_csv('synthetic_strava_data.csv')
df_clean = (df.sort_values("start_date")
              .drop_duplicates(["user_id", "route_id"], keep="last")
              .reset_index(drop=True))
df_enriched = engineer_rich_features(df_clean)

# Save
df_enriched.to_csv('processed_activities.csv', index=False)

# Aggregate routes
route_meta = df_enriched.groupby("route_id").agg({
    "surface_type_route": lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],
    "distance_km_route": "mean",
    # ... (add all columns from notebook Cell 14)
}).reset_index()

route_meta.to_csv('routes.csv', index=False)
```

---

## What the Streamlit App Expects

Based on `streamlit_app.py` comments:

### processed_activities.csv
- One row per activity
- Must have: user_id, route_id, rating, distance_km_route, elevation_meters_route, surface_type_route
- Used for: User profile building, activity history

### routes.csv
- One row per unique route
- Must have: route_id, distance_km_route, elevation_meters_route, surface_type_route, difficulty_score
- Used for: Route lookup, metadata display in recommendations

---

## Important Notes

### What the Notebook DOES
- ✅ Loads synthetic_strava_data.csv
- ✅ Cleans and deduplicates data
- ✅ Engineers 20+ features
- ✅ Aggregates to route level
- ✅ Creates feature matrices for ML models
- ✅ Trains recommendation models
- ✅ Evaluates model performance

### What the Notebook DOES NOT Do
- ❌ Export processed_activities.csv
- ❌ Export routes.csv
- ❌ Save data for Streamlit deployment
- ❌ Have explicit .to_csv() calls for these files

### Why CSV Generation is Needed

The notebook focuses on **model development and evaluation**. For **production deployment**, you need:
- Separate processed data files
- Lightweight format for Streamlit to load quickly
- Clean schema for UI display

The CSV files bridge the gap between the notebook's training pipeline and the web app's data requirements.

---

## Troubleshooting

### Error: "synthetic_strava_data.csv not found"
- Ensure synthetic_strava_data.csv exists in the project root
- Check path is correct if running from different directory

### Error: "KeyError: 'start_date'"
- Make sure you're using the correct synthetic data file
- Verify all required columns exist in the CSV

### Error: "NameError: engineer_rich_features is not defined"
- Make sure the generate_csv_exports.py script includes the function definition
- Or copy the function from Cell 12 of the notebook

### Processing takes too long
- If synthetic_strava_data.csv is very large (10k+ rows), processing may take 1-2 minutes
- This is normal; subsequent runs will be faster due to pandas optimization

### CSV files are too small
- If generated files have very few rows, check that synthetic_strava_data.csv has content
- Verify deduplication didn't remove too many rows

---

## Next Steps for Deployment

1. **Generate CSV files:**
   ```bash
   python generate_csv_exports.py
   ```

2. **Verify files were created:**
   ```bash
   ls -lh processed_activities.csv routes.csv
   ```

3. **Load data in Streamlit app:**
   - Uncomment code in streamlit_app.py
   - Update data loading to use these CSV files
   - Test with: `streamlit run streamlit_app.py`

4. **Deploy:**
   - Push to GitHub
   - Deploy to Streamlit Cloud or cloud platform
   - Set environment variables for Strava API credentials

---

## File Locations

- **Source data:** `/home/user/Strava-AI-Powered-Recommendations/synthetic_strava_data.csv`
- **Generation script:** `/home/user/Strava-AI-Powered-Recommendations/generate_csv_exports.py`
- **Output files:** Same directory (after running script)
- **Detailed analysis:** `/home/user/Strava-AI-Powered-Recommendations/DATA_SCHEMA_ANALYSIS.md`
- **Notebook:** `/home/user/Strava-AI-Powered-Recommendations/Strave_recommender_ver4.ipynb`

---

**Last Updated:** 2025-11-21
