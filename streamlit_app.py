# -*- coding: utf-8 -*-
"""streamlit_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CsxmBKi-5WuagFOTS_hcAhuB_q3uHW3M
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# import polyline
# import folium
# from streamlit_folium import st_folium
# import os
# import json
# from sklearn.metrics.pairwise import cosine_similarity
# from sklearn.preprocessing import MinMaxScaler
# from sklearn.feature_extraction.text import TfidfVectorizer
# import time
# 
# # --- Setup: Load Data (Mocked Data is implicitly loaded by the model logic) ---
# # NOTE: In a real app, you would save and load your model/data here.
# # For simplicity in this Colab, the core logic is self-contained.
# 
# # Load the external dataframes/variables saved from the previous execution cells
# # We re-create the dataframes and functions using the saved state from the notebook environment
# # This is a common pattern in Colab deployment when objects are created in a prior cell.
# # Since we can't share global memory easily in a written file, we'll use the hardcoded
# # ALL_USERS list for the UI, and the function re-runs the logic based on the full notebook scope.
# 
# ALL_USERS = ['anaisl', 'mrudulad', 'user_runner', 'user_biker'] # Hardcoded list of users
# 
# # Load the generated dataframes from CSV files
# try:
#     processed_df = pd.read_csv('processed_activities.csv')
#     routes_df = pd.read_csv('routes.csv')
#     # Ensure start_date is datetime
#     processed_df['start_date'] = pd.to_datetime(processed_df['start_date'])
# except FileNotFoundError:
#     st.error("Data files not found. Please run the data generation cell first.")
#     st.stop()
# 
# 
# # Re-define the complex functions/logic that were run in the main notebook cells
# # 1. Create the Route-Feature Matrix (The 'Content' Vector)
# route_features_df = processed_df[['route_id', 'distance_km_activity', 'elevation_meters_activity', 'surface_type']].drop_duplicates(subset=['route_id']).set_index('route_id')
# 
# # One-Hot Encode categorical features
# route_features_encoded = pd.get_dummies(route_features_df, columns=['surface_type'])
# 
# # Normalize numerical features (Distance and Elevation)
# scaler = MinMaxScaler()
# numerical_cols = ['distance_km_activity', 'elevation_meters_activity']
# route_features_encoded[numerical_cols] = scaler.fit_transform(route_features_encoded[numerical_cols])
# 
# # 2. Calculate Item-Item Similarity Matrix (Cosine Similarity)
# route_vectors = route_features_encoded.values
# item_similarity_matrix = cosine_similarity(route_vectors)
# route_map = {route_id: i for i, route_id in enumerate(route_features_encoded.index)}
# inverse_route_map = {i: route_id for route_id, i in route_map.items()}
# 
# # Function to get recommendations
# def get_personalized_recommendations(user_id, desired_distance, time_of_day, k=5):
#     """Generates recommendations based on user history and dynamic filters."""
# 
#     # 1. Identify routes the user rated highly (Score 4 or 5)
#     user_ratings = processed_df[(processed_df['user_id'] == user_id) & (processed_df['rating'] >= 4)]
# 
#     if user_ratings.empty:
#         # Fallback to the most popular routes if no high ratings exist (Cold Start)
#         most_popular = processed_df.groupby('route_id')['rating'].mean().sort_values(ascending=False).head(k * 2).index.tolist()
#         final_recommendations = routes_df[routes_df['route_id'].isin(most_popular)].head(k)
# 
#         final_recommendations['logic'] = "Cold Start: Recommended popular routes."
#         final_recommendations['score'] = np.random.uniform(70, 80, k).round(1)
#         return final_recommendations
# 
#     # 2. Compute the weighted average preference vector for the user
#     preferred_routes = user_ratings['route_id'].unique()
# 
#     # Calculate weighted similarity scores for all routes
#     sim_scores = {}
#     for route_id in route_features_encoded.index:
#         if route_id not in preferred_routes:
#             # Sum of similarity scores to all preferred routes
#             index = route_map[route_id]
#             sim_scores[route_id] = sum(item_similarity_matrix[index][route_map[pref_id]]
#                                       for pref_id in preferred_routes)
# 
#     # 3. Create a DataFrame from similarity scores and combine with route features
#     recommendation_scores = pd.DataFrame(list(sim_scores.items()), columns=['route_id', 'similarity_score'])
#     recommendation_scores = pd.merge(recommendation_scores, routes_df, on='route_id')
# 
#     # 4. Apply Dynamic Contextual Filtering (Distance & Time of Day)
# 
#     # Filter 1: Distance proximity (within 3km of desired distance)
#     recommendation_scores['distance_diff'] = abs(recommendation_scores['distance_km_route'] - desired_distance)
#     recommendation_scores = recommendation_scores[recommendation_scores['distance_diff'] <= 3]
# 
#     # Filter 2: Time of Day Preference (Mocked, as true 'time' data is hard to derive from context alone)
#     # We use the user's historical preference for the selected time slot
#     processed_df['time_category'] = processed_df['start_date'].dt.hour.apply(lambda hour: 'Morning (5-9 AM)' if 5 <= hour < 9 else ('Midday (9 AM - 2 PM)' if 9 <= hour < 14 else 'Evening (After 5 PM)'))
#     time_preference_score = processed_df[processed_df['user_id'] == user_id]['time_category'].value_counts(normalize=True).get(time_of_day, 0.5)
#     recommendation_scores['context_boost'] = recommendation_scores['similarity_score'] * time_preference_score
# 
# 
#     # Final Ranking
#     final_recommendations = recommendation_scores.sort_values(by=['context_boost', 'similarity_score'], ascending=False).head(k)
# 
#     # Add display columns
#     final_recommendations['logic'] = "Content-Based: Matched pace, distance, and surface preferences."
#     final_recommendations['score'] = (final_recommendations['context_boost'] * 100).round(1)
# 
#     return final_recommendations
# 
# # --- 3. WIDGET AND MAP LOGIC ---
# 
# st.title("ðŸƒâ€â™€ï¸ AI-Powered Personalized Activity Recommender Prototype ðŸš´")
# st.markdown("A Capstone Project by Anais Lacreuse & Mrudula Dama")
# st.markdown("---")
# 
# # --- SIDEBAR (The Interactive Widget) ---
# st.sidebar.header("ðŸŽ¯ Adjust Parameters")
# 
# selected_user = st.sidebar.selectbox(
#     "1. Select a User Profile:",
#     options=ALL_USERS,
#     index=0
# )
# 
# desired_distance = st.sidebar.slider(
#     "2. Desired Distance (km):",
#     min_value=5.0,
#     max_value=40.0,
#     value=15.0,
#     step=0.5
# )
# 
# time_of_day = st.sidebar.radio(
#     "3. Preferred Time of Day:",
#     options=['Morning (5-9 AM)', 'Midday (9 AM - 2 PM)', 'Evening (After 5 PM)'],
#     index=2
# )
# 
# # --- MAIN PAGE (Recommendation Output) ---
# 
# st.header(f"Recommendations for: **{selected_user}**")
# 
# # Run the recommendation function with the selected parameters
# top_recommendations = get_personalized_recommendations(
#     user_id=selected_user,
#     desired_distance=desired_distance,
#     time_of_day=time_of_day
# )
# 
# st.success("âœ… Top 5 Personalized Routes Generated (Content-Based Filtering)")
# 
# # Display the results table (excluding the polyline for cleanliness)
# display_cols = ['route_id', 'distance_km_route', 'elevation_meters_route', 'surface_type', 'logic']
# 
# st.dataframe(
#     top_recommendations[display_cols].style.format({
#         'distance_km_route': '{:.1f} km',
#         'elevation_meters_route': '{:,.0f} m',
#     }),
#     use_container_width=True,
#     column_config={"logic": st.column_config.Column("Recommendation Logic", width="large")}
# )
# 
# # --- Map Visualization (Top Recommended Route) ---
# 
# st.markdown("---")
# st.subheader(f"ðŸ—ºï¸ Visualization of Top Route: {top_recommendations.iloc[0]['route_id']}")
# 
# # Get the polyline for the first recommended route
# top_polyline = top_recommendations.iloc[0]['mock_encoded_polyline']
# 
# if top_polyline and top_polyline != 'N/A':
#     try:
#         decoded_coordinates = polyline.decode(top_polyline)
# 
#         # Determine map center
#         center_lat = np.mean([lat for lat, lon in decoded_coordinates])
#         center_lon = np.mean([lon for lat, lon in decoded_coordinates])
# 
#         # Initialize the Folium Map
#         m = folium.Map(
#             location=[center_lat, center_lon],
#             zoom_start=13,
#             tiles="CartoDB Positron"
#         )
# 
#         # Add the route as a PolyLine
#         folium.PolyLine(
#             decoded_coordinates,
#             color='#FF4B4B',  # Streamlit Red
#             weight=6,
#             opacity=0.9,
#             tooltip=f"Route: {top_recommendations.iloc[0]['distance_km_route']:.1f} km"
#         ).add_to(m)
# 
#         # Add start/end markers
#         folium.Marker(decoded_coordinates[0], tooltip='Start', icon=folium.Icon(color='green', icon='play')).add_to(m)
#         folium.Marker(decoded_coordinates[-1], tooltip='End', icon=folium.Icon(color='red', icon='stop')).add_to(m)
# 
#         # Display the map using the Streamlit component
#         st_folium(m, height=400, width="100%")
# 
#     except Exception as e:
#         st.error(f"Error rendering map: {e}. Check polyline data.")
# else:
#     st.info("Cannot visualize route. No valid GPS data available for this mock route.")
# 
# # --- Stretch Goal Mock-up ---
# 
# st.markdown("---")
# st.subheader("ðŸ”” Stretch Goal Demo: Proximity Alert Log")
# st.code(
#     """
#     [10:15 AM] ALERT: User 'jake_spence' finished a run 1.2 km from your location.
#     [08:45 AM] LOG: Webhook received. Activity created by user_runner.
#     [06:30 AM] ALERT: User 'anaisl' started an activity near your home at 6:25 AM!
#     """
# )
# st.caption("This feature requires a persistent Flask/FastAPI server with Strava Webhooks and a real-time distance calculation (geopy).")